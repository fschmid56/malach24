{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa as liro\n",
    "import librosa.display as lirod\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "from scipy.fft import rfft, rfftfreq\n",
    "import numpy as np\n",
    "\n",
    "wd = os.getcwd()\n",
    "os.chdir(os.path.join(wd, \"..\"))\n",
    "\n",
    "from models.mel import AugmentMelSTFT\n",
    "from datasets import audiodataset\n",
    "\n",
    "os.chdir(wd)\n",
    "\n",
    "example_data_path = os.path.join(\"..\", \"datasets\", \"example_data\", \"audio\")\n",
    "example_file = os.path.join(example_data_path, \"dog_bark.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Today's Menu\n",
    "\n",
    "- **Introduction to Pipeline**\n",
    "    * Tips & Tricks for building an ML pipeline\n",
    "    * Audio Preprocessing\n",
    "    * Example Pipeline on GitHub\n",
    "- **Your next task**\n",
    "- **Questions**\n",
    "- **Your presentation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Are we complete?\n",
    "\n",
    "* Team Task 1?\n",
    "* Team Task 6?\n",
    "* Team Task 8?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Pipeline - Expectation\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"figs/expectation.jpg\" alt=\"drawing\" width=\"600\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Pipeline - Reality\n",
    "\n",
    "<center>\n",
    "<img src=\"figs/reality.jpg\" alt=\"drawing\" width=\"600\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Tips & Tricks for building an ML pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Best Practices\n",
    "\n",
    "* **document everything** (you in two weeks and your colleagues should understand what you have done)\n",
    "* be careful and maybe **read things twice** (bugs in your pipeline may cost you hours later on)\n",
    "* **log as much as you can**\n",
    "* use **version control** - commit early, commit often"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Documentation\n",
    "\n",
    "* keep a continuously updated **work log** - your colleagues should be able to read up on what you have done quickly\n",
    "* a well-written work log will save you a lot of time when writing your **technical report**\n",
    "* put your work log in version control (at least a google doc for collaborating with your colleagues)\n",
    "* collect questions for us in your work log\n",
    "* add enough comments to your code, **explaining \"the why\" of it**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Version Control\n",
    "\n",
    "* use **git**\n",
    "* get familiar with it\n",
    "* use one repo for code, one for work log\n",
    "* when collaborating, use your own **branches**\n",
    "* **merge frequently**\n",
    "* keep your master branch stable and runnable\n",
    "* check new features added in a branch together in the group before merging them into the master branch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Testing\n",
    "\n",
    "run regular tests, in particular:\n",
    "* **acquaintance tests**: get to know a new library you don't know yet, allows you to learn a new API quickly, use e.g. a Jupyter Notebook\n",
    "* **unit tests**: simple tests for your own functions (e.g. data augmentation functions)\n",
    "* **integration tests**: more complex, test the interactions between your functions\n",
    "* **validity tests**: manual inspection (e.g., by looking at computed spectrograms) to check if your code works as expected\n",
    "\n",
    "**Try to verify each new code part in your group before running full experiments.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Reproducibility\n",
    "\n",
    "* **Code**: be aware of dependencies, use virtual environments (conda) and include frozen dependencies in version control\n",
    "* **Training**: use a mother seed sequence to seed all pseudo random number generators (torch, numpy, python random, ...), log seed, log commit-hash\n",
    "* **Data**: make preprocessing of data and labels repeatable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Leakage\n",
    "\n",
    "* **use the provided split for your task**!\n",
    "* make sure **NO** aspects of your test data distribution are leaking into your training or validation set\n",
    "* e.g., leaking through normalization is a prominent case\n",
    "\n",
    "<center>\n",
    "<img src=\"figs/leakage.png\" alt=\"drawing\" width=\"600\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Leakage across devices\n",
    "\n",
    "* e.g., in DCASE Task 1, **recording devices** play an important role\n",
    "* **generalization performance to unseen devices** should be measured\n",
    "* make sure to use the provided split; splitting data without paying attention to the devices will give you an incorrect estimation of your actual test performance\n",
    "\n",
    "<center>\n",
    "<img src=\"figs/leakage_devices.png\" alt=\"drawing\" width=\"600\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Audio Preprocessing\n",
    "\n",
    "#### Disclaimer:\n",
    "\n",
    "The following ...\n",
    "* is a short intro to selected topics relevant for audio signal processing. We try to give you an idea about the most important steps and hyperparameters. The purpose of this is to make you capable of understanding hyperparameter choices in related works and setting up your first Audio Deep Learning pipeline.\n",
    "* ... **does not** cover deep mathematical background.\n",
    "* ... **does not** give you the knowledge you would obtain by attending a signal processing course.\n",
    "\n",
    "If you want to dive deeper into signal processing, checkout <a href=https://www.dspguide.com/>this book</a>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Overview\n",
    "\n",
    "Audio signals are almost always converted to audio-visual representation, a.k.a., **spectrograms**. \n",
    "\n",
    "Common preprocessing steps (see <a href=https://github.com/fschmid56/malach24> example pipeline on GitHub </a>) involve:\n",
    "\n",
    "* **Pre-emphasis Filter**: time-domain finite impulse response (FIR) filter to modify the average spectral shape\n",
    "* **Short Time Fourier Transform**: convert time domain signal into time-frequency domain\n",
    "* Calculate **power** spectrogram (raise **magnitude** spectrogram to a certain power, e.g., 2)\n",
    "* Apply a **mel filterbank** to the spectrogram: match the human perception of frequency\n",
    "* Use logarithm $\\rightarrow$ **log mel spectrogram**: match human perception of loudness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Long story short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "ename": "LibsndfileError",
     "evalue": "Error opening '..\\\\datasets\\\\example_data\\\\audio\\\\dog_bark.wav': System error.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLibsndfileError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchaudio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms\n\u001b[1;32m----> 3\u001b[0m waveform, sample_rate \u001b[38;5;241m=\u001b[39m \u001b[43mtorchaudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mMelSpectrogram(sample_rate, n_fft\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m800\u001b[39m,\n\u001b[0;32m      5\u001b[0m                                       n_mels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m)\n\u001b[0;32m      6\u001b[0m mel_specgram \u001b[38;5;241m=\u001b[39m transform(waveform)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\malach24\\Lib\\site-packages\\torchaudio\\_backend\\utils.py:205\u001b[0m, in \u001b[0;36mget_load_func.<locals>.load\u001b[1;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load audio data from source.\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03mBy default (``normalize=True``, ``channels_first=True``), this function returns Tensor with\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;124;03m        `[channel, time]` else `[time, channel]`.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    204\u001b[0m backend \u001b[38;5;241m=\u001b[39m dispatcher(uri, \u001b[38;5;28mformat\u001b[39m, backend)\n\u001b[1;32m--> 205\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\malach24\\Lib\\site-packages\\torchaudio\\_backend\\soundfile.py:27\u001b[0m, in \u001b[0;36mSoundfileBackend.load\u001b[1;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m     19\u001b[0m     uri: Union[BinaryIO, \u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     25\u001b[0m     buffer_size: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4096\u001b[39m,\n\u001b[0;32m     26\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m---> 27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msoundfile_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\malach24\\Lib\\site-packages\\torchaudio\\_backend\\soundfile_backend.py:221\u001b[0m, in \u001b[0;36mload\u001b[1;34m(filepath, frame_offset, num_frames, normalize, channels_first, format)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;129m@_requires_soundfile\u001b[39m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m    141\u001b[0m     filepath: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28mformat\u001b[39m: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    147\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;28mint\u001b[39m]:\n\u001b[0;32m    148\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load audio data from file.\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \n\u001b[0;32m    150\u001b[0m \u001b[38;5;124;03m    Note:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;124;03m            `[channel, time]` else `[time, channel]`.\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 221\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43msoundfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSoundFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file_:\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m file_\u001b[38;5;241m.\u001b[39mformat \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWAV\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m normalize:\n\u001b[0;32m    223\u001b[0m             dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\malach24\\Lib\\site-packages\\soundfile.py:658\u001b[0m, in \u001b[0;36mSoundFile.__init__\u001b[1;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001b[0m\n\u001b[0;32m    655\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m mode\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info \u001b[38;5;241m=\u001b[39m _create_info_struct(file, mode, samplerate, channels,\n\u001b[0;32m    657\u001b[0m                                  \u001b[38;5;28mformat\u001b[39m, subtype, endian)\n\u001b[1;32m--> 658\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_int\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosefd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    659\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(mode)\u001b[38;5;241m.\u001b[39missuperset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseekable():\n\u001b[0;32m    660\u001b[0m     \u001b[38;5;66;03m# Move write position to 0 (like in Python file objects)\u001b[39;00m\n\u001b[0;32m    661\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\malach24\\Lib\\site-packages\\soundfile.py:1216\u001b[0m, in \u001b[0;36mSoundFile._open\u001b[1;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[0;32m   1213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_ptr \u001b[38;5;241m==\u001b[39m _ffi\u001b[38;5;241m.\u001b[39mNULL:\n\u001b[0;32m   1214\u001b[0m     \u001b[38;5;66;03m# get the actual error code\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m     err \u001b[38;5;241m=\u001b[39m _snd\u001b[38;5;241m.\u001b[39msf_error(file_ptr)\n\u001b[1;32m-> 1216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LibsndfileError(err, prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError opening \u001b[39m\u001b[38;5;132;01m{0!r}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname))\n\u001b[0;32m   1217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode_int \u001b[38;5;241m==\u001b[39m _snd\u001b[38;5;241m.\u001b[39mSFM_WRITE:\n\u001b[0;32m   1218\u001b[0m     \u001b[38;5;66;03m# Due to a bug in libsndfile version <= 1.0.25, frames != 0\u001b[39;00m\n\u001b[0;32m   1219\u001b[0m     \u001b[38;5;66;03m# when opening a named pipe in SFM_WRITE mode.\u001b[39;00m\n\u001b[0;32m   1220\u001b[0m     \u001b[38;5;66;03m# See http://github.com/erikd/libsndfile/issues/77.\u001b[39;00m\n\u001b[0;32m   1221\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mframes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mLibsndfileError\u001b[0m: Error opening '..\\\\datasets\\\\example_data\\\\audio\\\\dog_bark.wav': System error."
     ]
    }
   ],
   "source": [
    "from torchaudio import transforms\n",
    "\n",
    "waveform, sample_rate = torchaudio.load(example_file)\n",
    "transform = transforms.MelSpectrogram(sample_rate, n_fft=800,\n",
    "                                      n_mels=80)\n",
    "mel_specgram = transform(waveform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Convert to dB scale for better visualization\n",
    "mel_specgram_db = transforms.AmplitudeToDB()(mel_specgram[:, :, :300])\n",
    "\n",
    "# Plot the Mel spectrogram\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "plt.imshow(mel_specgram_db[0].numpy(), cmap='magma', origin='lower')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Mel Bin')\n",
    "plt.title('Mel Spectrogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Our Starting Point\n",
    "\n",
    "* **Sound**: variation in air pressure at a point in space as a function of time\n",
    "* The microphone turns the mechanical energy of a soundwave into an electrical signal\n",
    "* Analog signal is converted into a digital signal in two steps: **discretization at regular time intervals**, **quantization/rounding** of each sample to a fixed set of values\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"figs/digitized_signal.png\" alt=\"drawing\" width=\"300\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The sampling theorem\n",
    "\n",
    "* **a continuous (analog) signal can only be properly sampled if it does not contain frequency components above one-half of the sampling rate**\n",
    "* from a **properly sampled** signal, we can reconstruct the original signal\n",
    "* e.g., for a signal sampled at 44100 Hz, the **Nyquist frequency** is 22050 Hz \n",
    "* frequencies above $\\frac{1}{2}$ of the sampling rate are aliased into frequencies that can be represented in the digitized signal $\\rightarrow$ **information loss** \n",
    "* use analog low pass filter before sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The sampling theorem\n",
    "\n",
    "<center>\n",
    "<img src=\"figs/sampling_theorem.png\" alt=\"drawing\" width=\"700\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Digital Filters\n",
    "\n",
    "* digital filters are fundamental for digital audio processing\n",
    "* can be used in time or frequency domain\n",
    "* can be used to separate mixed signals, restore clean from distorted signals\n",
    "* are characterized by their **impulse/frequency response**\n",
    "* finite and infinite impulse response filters (FIR, IIR)\n",
    "* **FIR filters implemented using convolution with impulse response**\n",
    "* see **pre-emphasis filter** in example pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Discrete Fourier Transform\n",
    "\n",
    "* Fourier Analysis for **discrete time, discrete frequency, periodic** signals\n",
    "* we pretend our digitized signal repeats infinitely often in both directions \n",
    "* commonly the Fourier Transform is defined in the complex domain\n",
    "\n",
    "$$X[k]=\\frac{1}{N} \\sum_{n=0}^{N-1}x[n] \\mathrm{e}^{\\dfrac{-2\\pi k i n}{N}}$$\n",
    "\n",
    "* using Euler's formula $e^{ix}=\\cos x + i \\sin x$, we can think of the frequency domain as a set of sine and cosine wave amplitudes\n",
    "\n",
    "$$X[k]=\\frac{1}{N} \\sum_{n=0}^{N-1} \\underbrace{x[n] \\cos(\\frac{-2\\pi k n}{N})}_{\\textrm{real part}} + \\underbrace{i x[n] \\sin(\\frac{-2 \\pi k n}{N})}_{\\textrm{imaginary part}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Discrete Fourier Transform\n",
    "\n",
    "Basis functions for 16-point DFT.\n",
    "\n",
    "<center>\n",
    "<img src=\"figs/basis.png\" alt=\"drawing\" width=\"400\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Discrete Fourier Transform\n",
    "\n",
    "* time domain signal runs from $0$ to $N-1$\n",
    "* translates to $\\frac{N}{2} + 1$ sine (imaginary) and cosine (real) amplitudes\n",
    "* DFT can be computed by matrix multiplication with basis function ($O(N^2)$)\n",
    "* ... or much faster using **FFT** ($O(N log N)$)\n",
    "\n",
    "<center>\n",
    "<img src=\"figs/DFT.png\" alt=\"drawing\" width=\"500\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Magnitude and Phase\n",
    "\n",
    "* Cosine and Sine wave amplitudes can be converted to **polar form**:\n",
    "* **Magnitude**: $Mag X[k] = \\sqrt{(Re X[k])^2 + (Im X[k])^2}$\n",
    "* **Phase**: contains little useful information; human ear is insensitive to the relative phase of component sinusoids\n",
    "* In a magnitude spectrum, the **magnitude of a bin $X[k]$ is the sum of all energy in its frequency band**\n",
    "\n",
    "<center>\n",
    "<img src=\"figs/mag_phase.png\" alt=\"drawing\" width=\"400\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Frequency Resolution\n",
    "\n",
    "* N-point FFT (N usually chosen as a power of two; signal is zero-padded) results in spectrum with $\\frac{N}{2} + 1$ frequency bins\n",
    "* bins are equally spaced in the range $[0, \\frac{S}{2}]$ Hertz (S is sampling rate)\n",
    "* the frequency resolution (in Hertz) is $r \\approx \\frac{S}{N}$\n",
    "* can **reduce inter-sample spacing in frequency domain by zero padding** (increase resolution in frequency domain)\n",
    "\n",
    "<center>\n",
    "<img src=\"figs/zero_padding.png\" alt=\"drawing\" width=\"600\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Spectral Leakage and Windowing\n",
    "\n",
    "* DFT assumes a **periodic signal** $\\rightarrow$ our time-domain signal is **implicitly repeated**\n",
    "\n",
    "<center>\n",
    "<img src=\"figs/signal_repeated.png\" alt=\"drawing\" width=\"800\"/>\n",
    "</center>\n",
    "\n",
    "* a sine NOT matching a basis function will be cut at the border; **explaining this cut requires all the basis functions**\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"figs/spectral_leakage.png\" alt=\"drawing\" width=\"400\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Spectral Leakage and Windowing\n",
    "\n",
    "* signals can be multiplied with a window to **avoid hard cuts at the borders**\n",
    "* results in comparable peaks for sines matching or not matching a basis function\n",
    "* popular windows: Hann, Hamming, Gaussian, etc.\n",
    "* trade-off between **width of the peak** (resolution) and **spectral leakage** (amplitude of tails)\n",
    "\n",
    "<center>\n",
    "<img src=\"figs/windowing.png\" alt=\"drawing\" width=\"600\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Short-Time Fourier Transform\n",
    "\n",
    "* computing DFT of the whole audio clip, we **lose all timing information**\n",
    "* audio clip is cut up in **overlapping frames**, and **DFT is computed per frame**\n",
    "* **window length**: make frames long enough to have a **useful frequency resolution** and short enough such that the **signal is stationary within a frame**\n",
    "\n",
    "<center>\n",
    "<img src=\"figs/stft.png\" alt=\"drawing\" width=\"400\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Mel Spectrogram\n",
    "\n",
    "* we usually **apply additional filterbanks** on the spectrogram computed by STFT\n",
    "* can be viewed as a matrix multiplication: $\\mathrm{torch.matmul(filterbank, spectrogram)}$\n",
    "* want to **decrease dimensionality** and keep relevant audio features for task $\\rightarrow$ idea is to model human perception\n",
    "* humans don't perceive frequencies on a **linear scale**; **detecting differences in lower frequencies is easier than in higher frequencies**\n",
    "* want to have higher resolution in low-frequency regions, lower resolution in high-frequency regions\n",
    "* **Mel scale**: equal distances on mel scale sound equally distant to listeners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Mel Spectrogram\n",
    "\n",
    "<center>\n",
    "<img src=\"figs/mel_filterbank.png\" alt=\"drawing\" width=\"600\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def wav_plot(x, sr, listen=True, title=\"\"):\n",
    "    fig, ax = plt.subplots(nrows=1, figsize=(10, 2))\n",
    "    ax.set_title(title)\n",
    "    lirod.waveshow(x, sr=sr, ax=ax, axis='s')\n",
    "    plt.show()\n",
    "    if listen:\n",
    "        audio = ipd.Audio(x, rate=sr)\n",
    "        ipd.display(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def freq_plot(x, sr, title=\"\", n=1024, log_freq=True):\n",
    "    x_mag = np.abs(rfft(x, n=n))\n",
    "    x_mag = liro.amplitude_to_db(x_mag)\n",
    "    freqs = rfftfreq(n, 1 / sr)\n",
    "    fig, ax = plt.subplots(nrows=1, figsize=(10, 2))\n",
    "    ax.set_title(title)\n",
    "    if log_freq:\n",
    "        ax.plot(freqs, x_mag)\n",
    "        ax.set_xscale('symlog', base=2)\n",
    "    else:\n",
    "        ax.plot(x_mag)\n",
    "    ax.set_xlabel(\"Frequency (Hertz)\")\n",
    "    ax.set_ylabel(\"Amplitude (dB)\")\n",
    "    ax.xaxis.set_major_formatter(matplotlib.ticker.ScalarFormatter())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def spec_liro(x, sr, n_fft=1024, win_length=800, hop_length=320, x_is_spec=False,\n",
    "              x_is_power_spec=False, x_is_mel_spec=False, convert_to_db=True, title=\"Spectrogram\"):\n",
    "    fig, ax = plt.subplots(nrows=1, figsize=(10, 2))\n",
    "    ax.set_title(title)\n",
    "    if x_is_mel_spec:\n",
    "        x_is_power_spec = True\n",
    "        x_is_spec = True\n",
    "    if x_is_power_spec:\n",
    "        x_is_spec = True\n",
    "    if not x_is_spec:\n",
    "        x = liro.stft(x, n_fft=n_fft, win_length=win_length, hop_length=hop_length)\n",
    "        x, phase = liro.magphase(x)\n",
    "    if convert_to_db:\n",
    "        if x_is_power_spec:\n",
    "            # power spectrogram \n",
    "            x = liro.power_to_db(x)\n",
    "        else:\n",
    "            # magnitude spectrogram\n",
    "            x = liro.amplitude_to_db(x) \n",
    "    img = lirod.specshow(\n",
    "        x,\n",
    "        sr=sr,\n",
    "        x_axis='s',\n",
    "        y_axis='mel' if x_is_mel_spec else 'log',\n",
    "        cmap='magma',\n",
    "        ax=ax,\n",
    "        n_fft=n_fft,\n",
    "        win_length=win_length\n",
    "    )\n",
    "    fig.colorbar(img, ax=ax, format='%+2.0f dB' if convert_to_db else None)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## An Example: from the waveform to the log mel spectrogram\n",
    "\n",
    "Our starting point: ... a dog barking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sr = 32000 \n",
    "example_wav, _ = liro.load(example_file, sr=sr)\n",
    "wav_plot(example_wav, sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Apply Pre-emphasis (Digital Filter)\n",
    "\n",
    "* Lower frequencies tend to carry more energy than higher frequencies (drops around 2 dB/kHz)\n",
    "* Pre-emphasis **compensates for the average spectral shape** and **emphasizes higher frequencies**\n",
    "* implemented as **time-domain FIR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "preemphasis_coefficient = torch.as_tensor([[[-.97, 1]]])\n",
    "wav_torch = torch.from_numpy(example_wav)\n",
    "wav_pree = nn.functional.conv1d(wav_torch.reshape(1, 1, -1), preemphasis_coefficient).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "freq_plot(preemphasis_coefficient.squeeze().numpy(), sr, title=\"Pre-emphasis Filter Frequency Magnitude Reponse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "spec_liro(example_wav, sr, title=\"Log Spectrogram (without Pre-emphasis)\")\n",
    "spec_liro(wav_pree.squeeze().numpy(), sr, title=\"Log Spectrogram (with Pre-emphasis)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## STFT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "n_fft, win_length, hop_length = 1024, 800, 320 \n",
    "window = torch.hann_window(win_length, periodic=False)\n",
    "spec = torch.stft(wav_pree, n_fft=n_fft, hop_length=hop_length, win_length=win_length,\n",
    "                       center=True, normalized=False, window=window, return_complex=True)\n",
    "print(\"Complex spec shape: \", spec.shape)\n",
    "spec = torch.view_as_real(spec)\n",
    "print(\"Real spec shape: \", spec.shape)\n",
    "power_spec = (spec ** 2).sum(dim=-1)\n",
    "# for comparison, we calculate also the magnitude spectrogram\n",
    "mag_spec = torch.sqrt(power_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## STFT Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "wav_plot(window.squeeze().numpy(), sr, listen=False, title=\"Hann window (time-domain)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "spec_liro(mag_spec.squeeze().numpy(), sr, x_is_spec=True, convert_to_db=False, title=\"Magnitude Spectrogram\")\n",
    "spec_liro(power_spec.squeeze().numpy(), sr, x_is_power_spec=True, convert_to_db=False, title=\"Power Spectrogram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Mel Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "n_mels, fmin, fmax = 40, 0.0, sr // 2\n",
    "mel_basis, _ = torchaudio.compliance.kaldi.get_mel_banks(n_mels, n_fft, sr,\n",
    "                                                                 fmin, fmax, vtln_low=100.0, vtln_high=-500.,\n",
    "                                                                 vtln_warp_factor=1.0)\n",
    "# pad with one zero per mel bin to match n_fft // 2 + 1\n",
    "mel_basis = torch.as_tensor(torch.nn.functional.pad(mel_basis, (0, 1), mode='constant', value=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, figsize=(10, 4))\n",
    "ax.set_title(\"Mel filterbank\")\n",
    "ax.set_xlabel(\"FFT bin index\")\n",
    "ax.set_ylabel(\"Mel bin\")\n",
    "ax.imshow(mel_basis.squeeze().numpy(), cmap='hot', interpolation='nearest', aspect='auto')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "melspec = torch.matmul(mel_basis, power_spec)\n",
    "spec_liro(power_spec.squeeze().numpy(), sr, x_is_power_spec=True, title=\"Log Power Spectrogram\")\n",
    "spec_liro(melspec.squeeze().numpy(), sr, x_is_mel_spec=True, title=\"Log Mel Spectrogram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##  Why the 'Log' in amplitude?\n",
    "\n",
    "**Perceived loudness** and **sound power** have an **logarithmic relationship**. Increasing the sound power by a factor of 10 increases the perceived loudness by about a factor of 2. We account for that by taking the logarithm of sound power in our preprocessing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "log_mel_spec = (melspec + 0.00001).log()\n",
    "spec_liro(melspec.squeeze().numpy(), sr, x_is_mel_spec=True, convert_to_db=False, title=\"Mel Spectrogram\")\n",
    "spec_liro(log_mel_spec.squeeze().numpy(), sr, x_is_mel_spec=True, convert_to_db=False, title=\"Log Mel Spectrogram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Normalization\n",
    "\n",
    "We often want to **normalize spectrograms by dataset mean and variance**. Below we use approximated mean (-4.5) and standard deviation (5). However, **running through the dataset** and calculating mean and standard deviation is the usual way to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "norm_log_mel_spec = (log_mel_spec + 4.5) / 5.\n",
    "spec_liro(log_mel_spec.squeeze().numpy(), sr, x_is_mel_spec=True, convert_to_db=False, title=\"Log Mel Spectrogram\")\n",
    "spec_liro(norm_log_mel_spec.squeeze().numpy(), sr, x_is_mel_spec=True, convert_to_db=False, title=\"Norm. Log Mel Spectrogram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What to do with a normalized log mel spectrogram?\n",
    "\n",
    "**Use your favorite vision architecture and treat the normalized log mel spectrogram as an image with a single input channel.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example Pipeline on GitHub\n",
    "\n",
    "The <a href=https://github.com/fschmid56/malach23-pipeline> example ML4Audio pipeline </a> demonstrates the following points based on 200 wav files:\n",
    "\n",
    "* Dataset loading, PyTorch Dataset class, PyTorch Dataloader\n",
    "* our pre-processing routine that we discussed today\n",
    "* how to use a PyTorch Model (CNN) to generate predictions based on a log mel spectrogram\n",
    "* simple data augmentation techniques (masking time frames, masking frequency bands, mixup, time rolling)\n",
    "* a training loop implemented with PyTorch Lightening\n",
    "* logging implemented with Weights and Biases\n",
    "* **some** of the best practices we discussed today"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Your next goal\n",
    "\n",
    "* Settle for a particular approach (DCASE baseline system for your task, or system described in a technical report)\n",
    "* Pick an approach that you can handle w.r.t. to its (computational) complexity\n",
    "* Implement this approach in your own pipeline\n",
    "* Run a successful experiment and **show us the logged metrics**\n",
    "* Read the detailed task description as soon as it is out (1st of April)\n",
    "\n",
    "**Finish until 10.04.24** (after easter holidays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Until 20.03.24 (our next meeting)\n",
    "\n",
    "Prepare a presentation (up to 20 minutes) including:\n",
    "\n",
    "* detailed information about the system you are going to implement\n",
    "* the current state of your pipeline (how are you going to structure it?)\n",
    "* decision on which frameworks (e.g., pytorch, pytorch lightning, weights and biases) you are planning to use \n",
    "* possible technical difficulties in implementing the chosen approach that you would like to discuss with us\n",
    "\n",
    "You may use the <a href=https://github.com/fschmid56/malach24> example ML4Audio pipeline </a> to get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Questions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Do you feel comfortable with today's content?\n",
    "* Do you have computing resources available? Do you need resources from us?\n",
    "* More questions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Your Presentations"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
